{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img style=\"float: right\" src=\"images/surfsara.png\">\n",
    "<br/>\n",
    "<hr style=\"clear: both\" />\n",
    "\n",
    "# Machine Learning - Random forests in Spark\n",
    "In this notebook, we'll try to assess the credit risk on a German credit data set. Based on a number of _features_, listed below, we wil need to train a machine learning model that predicts whether a person can be safely offered credit or not.\n",
    "\n",
    "Please note that we follow the example of a [blog post on the MapR website](https://mapr.com/blog/predicting-loan-credit-risk-using-apache-spark-machine-learning-random-forests/). That example uses Scala, below we use Python.\n",
    "\n",
    "The structure of the data is shown in the table below. Notice that the first field is the _label_ (or _class_). This is the field we  will try to predict later. This value can be either 0 (false - high credit risk) or 1 (true - low credit risk). \n",
    "<br/>\n",
    "<br/>\n",
    "<img style=\"float: center\" width=\"70%\" src=\"https://mapr.com/blog/predicting-loan-credit-risk-using-apache-spark-machine-learning-random-forests/assets/blogimages/sparkmlgermancreditdata.png\">\n",
    "<br/>\n",
    "<br/>\n",
    "A more detailed description of the data, also listing the meaning of the attribute values, is available on the <a href=\"https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)\">Machine Learning Repository</a> from the University of California, Irvine.\n",
    "\n",
    "\n",
    "Because the label can only have two possible outcomes, this problem is known as a _binary classification problem_. To solve this problem, we must build a _classifier_ that takes all input features of the data set and successfully predicts the _label_ on new instances (persons we haven't seen before). \n",
    "\n",
    "Because the _label_ is known in advance, we will use a _supervised_ learning algorithm, in this case Random Forests. First, we use labeled data to train and build our model. This data contains features of each person and also the decision on creditability, the label that we have to predict for new data.\n",
    "\n",
    "Please note that this dataset is very clean. In general you will spend a lot of time preprocessing your data before using it to train a model. In this example, we will only perform minimal preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Decision trees\n",
    "A Random Forest consists of a number of _decision trees_ (hence the name). Before we dive into using Random Forests proper, we will study decision trees first to get a feel for how they work. We quote from the MapR blog post: \n",
    "\n",
    "\"Decision trees create a model that predicts the class or label based on several input features. Decision trees work by evaluating an expression containing a feature at every node and selecting a branch to the next node based on the answer. A possible decision tree for predicting Credit Risk is shown below. The feature questions are the nodes, and the answers “yes” or “no” are the branches in the tree to the child nodes.\"\n",
    "\n",
    "<img style=\"float: center\" width=\"80%\" src=\"https://mapr.com/blog/predicting-loan-credit-risk-using-apache-spark-machine-learning-random-forests/assets/blogimages/creditdecisiontree.png\">\n",
    "\n",
    "<br/>\n",
    "Random Forests are a generalization of this method. Instead of creating one tree we create many trees, each for a random portion of the data. When building trees we also make sure that they use a random subset of features . Our final model is the average tree based on all these smaller trees.\n",
    "\n",
    "If you want to know more you may enjoy this [video](https://www.youtube.com/watch?v=3kYujfDgmNk)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Spark creates all trees of a Random Forest in parallel. In this case the data set is small and the number of features and trees limited. But for many real time applications scaling this algorithm is very important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create a SparkSession, the 'DataFrame version' of the SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We read in the csv data. We will let Spark figure out the schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "credits_df = spark \\\n",
    "    .read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .load(\"../data/germancredit.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "credits_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's inspect the resulting DataFrame by converting it to a Pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "credits_df.toPandas().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Spark can provide us with some information on the distribution of features. Usually you would perform a more extensive exploration of the data first. For example, we often calculate correlations between features to investigate the underlying structure of the data. Here we do all of this very briefly.\n",
    "\n",
    "In the cell below, we calculate some descriptive statistics for the `amount` feature, which corresponds to the amount of credit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "credits_df.describe(\"amount\", \"age\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Below we do some more descriptive analytics on the relation between creditability and the amount of credit. First, we look at the average amount of credit per creditability class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "credits_df.groupBy(\"creditability\").agg({\"amount\" : \"avg\"}).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can visualise the amount of credit per class in a box plot using the [`seaborn`](https://seaborn.pydata.org/generated/seaborn.boxplot.html) plotting library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline\n",
    "\n",
    "mpl.rcParams['axes.labelsize'] = 24\n",
    "mpl.rcParams['xtick.labelsize'] = 18\n",
    "mpl.rcParams['ytick.labelsize'] = 18\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.boxplot(data=credits_df.toPandas(), x='creditability', y='amount')\n",
    "plt.ylabel('Amount of credit')\n",
    "plt.xlabel('Credit risk class - high risk vs. low risk');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can also create a table that lists the number of high risk and low risk persons by the status of their checking account (the `balance` feature):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# For information on the mapping between the checking account attribute's value and its meaning, see:\n",
    "# https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)\n",
    "\n",
    "CHECKING_ACCOUNT_STATUS = {\n",
    "    1: '< 0 DM',\n",
    "    2: '0 <= ... < 200 DM',\n",
    "    3: '... >= 200 DM',\n",
    "    4: 'no checking account'\n",
    "}\n",
    "\n",
    "checking_account_status = F.udf(lambda index: CHECKING_ACCOUNT_STATUS[index], StringType())\n",
    "\n",
    "credits_df \\\n",
    "    .groupBy('balance') \\\n",
    "    .pivot('creditability') \\\n",
    "    .count() \\\n",
    "    .withColumnRenamed('0', 'high_risk') \\\n",
    "    .withColumnRenamed('1', 'low_risk') \\\n",
    "    .orderBy('balance', ascending=False) \\\n",
    "    .withColumn('checking_account_status', checking_account_status('balance')) \\\n",
    "    .toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We will need to make a decision on what features to include for our model training. We put these in a list. Notice, that we choose all features. Normally a selection is made based on informative value and relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "feature_column_names = [\"balance\", \"duration\", \"history\", \"purpose\", \"amount\",\n",
    "               \"savings\", \"employment\", \"instPercent\", \"sexMarried\",\n",
    "               \"guarantors\", \"residenceDuration\", \"assets\",  \"age\", \n",
    "               \"concCredit\", \"apartment\", \"credits\",\"occupation\", \n",
    "               \"dependents\",  \"hasPhone\", \"foreign\" ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The algorithm wants the data set but also needs the feature in the form of a feature vector. Spark has a helper class for this: `VectorAssembler`. We use it to put our features in the dataset under the column `features`.\n",
    "\n",
    "If you execute the next cell and scroll to the right you can that the feature vector has been added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler  \n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_column_names, outputCol=\"features\")\n",
    "features_df = assembler.transform(credits_df)\n",
    "features_df.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Then we take the data set and split it randomly in a training part (80% of the data) and a test part (20%).\n",
    "We train our model on the training data and test our performance on the test data later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_data, test_data = features_df.randomSplit([0.8, 0.2], 12345)\n",
    "train_data.count(), test_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Then we can run our Random Forest algorithm. The next cell creates 20 trees, each using 5 features. Continuous variables are split into 150 bins, and we use a random seed. (This seed allow us to replay the algorithm whilst preserving randomness.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(numTrees=20, maxDepth=5, labelCol=\"creditability\", seed=42)\n",
    "model = rf.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Our model has been built. Its predictions are the average of those of the 20 trees that were built. We can now make predictions. In the next cell we predict the label of the test data set.\n",
    "\n",
    "The predictions show two probabilities, one for 0 (high credit risk) and one for 1 (low credit risk)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = model.transform(test_data)\n",
    "predictions.toPandas()[['probability', 'prediction']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To see how good the predictions are we can use an Evaluator. Below we determine the accuracy of the predictions on the test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='creditability')\n",
    "accuracy = evaluator.evaluate(predictions) \n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Model inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can plot the the importance of each feature for the final prediction using seaborn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "\n",
    "importances_df = pd \\\n",
    "    .DataFrame({'importance': model.featureImportances.toArray(), 'feature': feature_column_names}) \\\n",
    "    .sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(16, 8))    \n",
    "sns.barplot(data=importances_df, x='feature', y='importance')\n",
    "plt.xticks(rotation=90, fontsize=18);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can also print out one of the decision trees used in the Random Forest model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "tree_model_string = model.trees[0].toDebugString\n",
    "for index, column_name in enumerate(feature_column_names):\n",
    "    tree_model_string = tree_model_string.replace(\n",
    "        'feature {} '.format(index),\n",
    "        '{} '.format(column_name)\n",
    "    )\n",
    "\n",
    "print(tree_model_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
