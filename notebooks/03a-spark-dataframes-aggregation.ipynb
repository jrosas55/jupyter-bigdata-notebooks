{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left\" src=\"images/spark.png\" />\n",
    "<img style=\"float: right\" src=\"images/surfsara.png\" />\n",
    "<hr style=\"clear: both\" />\n",
    "\n",
    "## Spark Aggregation problem\n",
    "\n",
    "Below is a exercise for groupBy and aggregation on Dataframes in pySpark.\n",
    "\n",
    "The [groupBy](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.groupBy) method on a dataFrame does not return another dataFrame but a [groupedData]( https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData) object that has several handy aggregation methods, like [avg](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData.avg), [count](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData.count), [max](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData.max), [min](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData.min) and many others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem below was taken from Coursera's MOOC [Big Data Analysis with Scala and Spark](https://www.coursera.org/learn/scala-spark-big-data) by the Ecole Polytechnique Federale de Lausanne. We adapted the problem for pySpark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume we have a dataset about posts in a discussion forum. The entries of the dataset consist of an authorID, the name of a subforum, the number of likes and a date.<br>\n",
    "\n",
    "<b>We would like to tally up the all posts for each author and then rank authors with the most likes per subforum.</b>\n",
    "\n",
    "Aftre creating a SparkSession, we present the data as a python dictionary that we then transform to a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a SparkSession, the 'DataFrame version' of the SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  pyspark.sql import Row\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "\n",
    "posts = [{'authorID' : 4, 'subforum': 'java', 'likes': 5, 'date' : 'sept 5'},\n",
    "         {'authorID' : 1, 'subforum': 'python', 'likes': 3, 'date' : 'sept 4'},\n",
    "        {'authorID' : 2, 'subforum': 'python', 'likes': 35, 'date' : 'sept 3'},\n",
    "        {'authorID' : 3, 'subforum': 'java', 'likes': 1, 'date' : 'sept 5'},\n",
    "        {'authorID' : 4, 'subforum': 'java', 'likes': 14, 'date' : 'sept 5'},\n",
    "        {'authorID' : 3, 'subforum': 'python', 'likes': 12, 'date' : 'sept 3'},\n",
    "        {'authorID' : 3, 'subforum': 'java', 'likes': 14, 'date' : 'sept 5'},\n",
    "        {'authorID' : 3, 'subforum': 'java', 'likes': 10, 'date' : 'sept 5'},\n",
    "        {'authorID' : 2, 'subforum': 'python', 'likes': 21, 'date' : 'sept 5'}]\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(posts)\n",
    "dfPosts = spark.createDataFrame(rdd.map(lambda x : Row(**x)))\n",
    "dfPosts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPosts.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please use a [groupBy](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.groupBy), an [aggregation](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData.agg) and an [orderBy](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.orderBy) to come up with the desired dataFrame. Note that you want to order in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
